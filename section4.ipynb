{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 NLP and Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553f6ec1d3814abfaf590cf9d865d28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rohit\\.cache\\huggingface\\hub\\models--nlptown--bert-base-multilingual-uncased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ef48c504734f1985a4e3003b4f4353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c6715af526486abec888e49b11b817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b3307fd26f4ff69b44b0f9528f284b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0f2476a51b479fbcab64e9e080c25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '5 stars', 'score': 0.7150049209594727}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the text classification pipeline with a pre-trained BERT model\n",
    "classifier = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Input text\n",
    "text = \"The weather today is amazing!\"\n",
    "\n",
    "# Perform classification\n",
    "result = classifier(text)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '5 stars', 'score': 0.7150049209594727}]\n",
      "[{'label': '4 stars', 'score': 0.26384586095809937}]\n",
      "[{'label': '1 star', 'score': 0.2788242995738983}]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"The weather today is amazing!\"\n",
    "text2=\"text\"\n",
    "text3=\"copy code\"\n",
    "\n",
    "result1 = classifier(text1)\n",
    "result2 = classifier(text2)\n",
    "result3 = classifier(text3)\n",
    "# Print the result\n",
    "\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 What is fine-tuning in the context of LLMs? How is it different from transfer learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning vs. Transfer Learning in Large Language Models (LLMs)\n",
    "\n",
    "\n",
    "\n",
    "#### **Transfer Learning**\n",
    "\n",
    "- **Definition:** Transfer learning involves taking a model trained on one task (the **source task**) and applying its knowledge to a different but related task (the **target task**).\n",
    "- **Purpose:** It leverages existing knowledge to improve learning efficiency and performance on new tasks, especially when the target task has limited data.\n",
    "- **Methods:**\n",
    "  - **Feature Extraction:** Using the pre-trained model to extract meaningful features from data, which are then used in another model.\n",
    "  - **Fine-Tuning:** (See below) Adjusting the pre-trained model for the new task.\n",
    "\n",
    "#### **Fine-Tuning**\n",
    "\n",
    "- **Definition:** Fine-tuning is a specific type of transfer learning where a pre-trained model is further trained on a new, usually smaller, dataset tailored to a specific task.\n",
    "- **Purpose:** To adapt the general knowledge of the pre-trained model to perform optimally on a particular task.\n",
    "- **Process:**\n",
    "  - Start with a model pre-trained on a large dataset.\n",
    "  - Continue training the model on a task-specific dataset, allowing it to adjust its weights for better performance on that task.\n",
    "\n",
    "#### **Key Differences**\n",
    "\n",
    "| Aspect             | Transfer Learning                              | Fine-Tuning                                    |\n",
    "|--------------------|------------------------------------------------|------------------------------------------------|\n",
    "| **Scope**          | Broad strategy for applying knowledge to new tasks | Specific method within transfer learning      |\n",
    "| **Application**    | Includes feature extraction, model adaptation, etc. | Involves further training a pre-trained model |\n",
    "| **Flexibility**    | Encompasses various techniques                 | Focuses on adjusting model parameters          |\n",
    "| **Use Case Example** | Using a model trained on ImageNet for medical imaging | Continuing training BERT on a sentiment analysis dataset |\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
